# Data Warehouse Empresarial con Hadoop + Spark SQL
## ImplementaciÃ³n de Arquitectura Big Data para AnÃ¡lisis de Ventas

---

### ğŸ“‹ **RESUMEN EJECUTIVO**

Este proyecto implementa un **Data Warehouse empresarial** completo utilizando tecnologÃ­as Big Data (Hadoop + Spark SQL) para el anÃ¡lisis de datos de ventas de la empresa SuperVentas. Se desarrollÃ³ una arquitectura escalable que procesa datos desde fuentes CSV hasta un modelo dimensional optimizado para anÃ¡lisis de negocio.

**Resultado**: Sistema funcional que procesa 50+ registros y genera insights empresariales crÃ­ticos en tiempo real.

---

### ğŸ¯ **OBJETIVOS CUMPLIDOS**

âœ… **Infraestructura Big Data**: ConfiguraciÃ³n completa de ecosistema Hadoop  
âœ… **Data Lake**: ImplementaciÃ³n de arquitectura de datos moderna  
âœ… **ETL Pipeline**: Proceso automatizado de extracciÃ³n, transformaciÃ³n y carga  
âœ… **Modelo Dimensional**: Esquema estrella optimizado para anÃ¡lisis  
âœ… **Business Intelligence**: Consultas analÃ­ticas para toma de decisiones  

---

### ğŸ—ï¸ **ARQUITECTURA TÃ‰CNICA**

#### **Stack TecnolÃ³gico**
| Componente | VersiÃ³n | FunciÃ³n |
|------------|---------|---------|
| **Hadoop HDFS** | 3.4.1 | Almacenamiento distribuido |
| **Apache Spark** | 3.5.0 | Motor de procesamiento |
| **Spark SQL** | 3.5.0 | Engine de consultas analÃ­ticas |
| **Hive** | 4.0.1 | Metastore y compatibilidad |

#### **Flujo de Datos (Data Pipeline)**
```
ğŸ“Š Fuentes CSV â†’ ğŸ—„ï¸ HDFS Raw â†’ âš¡ Spark ETL â†’ ğŸ“ˆ Parquet DW â†’ ğŸ“Š Analytics
     (4 tablas)    (Data Lake)   (TransformaciÃ³n)  (Star Schema)   (BI Queries)
```

---

### ğŸ“ **ESTRUCTURA DEL DATA LAKE**

#### **Capa RAW (Datos Originales)**
```
ğŸ“‚ /datalake/raw/superventas/
â”œâ”€â”€ ğŸ‘¥ clientes/     â†’ InformaciÃ³n de clientes (10 registros)
â”œâ”€â”€ ğŸ“¦ productos/    â†’ CatÃ¡logo de productos (10 registros)  
â”œâ”€â”€ ğŸ¢ sucursales/   â†’ Red de sucursales (10 registros)
â””â”€â”€ ğŸ’° ventas/       â†’ Transacciones de venta (20 registros)
```

#### **Capa PROCESSED (Modelo Dimensional)**
```
ğŸ“‚ /datalake/processed/superventas/
â”œâ”€â”€ ğŸ”‘ dim_cliente/   â†’ DimensiÃ³n Cliente (10 registros)
â”œâ”€â”€ ğŸ”‘ dim_producto/  â†’ DimensiÃ³n Producto (10 registros)
â”œâ”€â”€ ğŸ”‘ dim_sucursal/  â†’ DimensiÃ³n Sucursal (10 registros)
â”œâ”€â”€ ğŸ”‘ dim_tiempo/    â†’ DimensiÃ³n Tiempo (20 registros)
â””â”€â”€ ğŸ“Š fact_ventas/   â†’ Tabla de Hechos (20 registros)
```

---

### ğŸ² **MODELO DIMENSIONAL (ESQUEMA ESTRELLA)**

#### **Dimensiones Implementadas**
| DimensiÃ³n | Atributos Clave | PropÃ³sito |
|-----------|-----------------|-----------|
| **dim_cliente** | sk_cliente, segmento, ciudad | AnÃ¡lisis por perfil de cliente |
| **dim_producto** | sk_producto, categoria, precio_lista | AnÃ¡lisis por lÃ­nea de producto |
| **dim_sucursal** | sk_sucursal, region, ciudad | AnÃ¡lisis geogrÃ¡fico |
| **dim_tiempo** | sk_tiempo, anio, mes, trimestre | AnÃ¡lisis temporal |

#### **Tabla de Hechos**
- **fact_ventas**: MÃ©tricas centrales (cantidad, precio_unit, total_linea)
- **Claves ForÃ¡neas**: ConexiÃ³n con todas las dimensiones
- **Medidas Calculadas**: Total por lÃ­nea, agregaciones automÃ¡ticas

---

### ğŸ”§ **COMPONENTES FUNCIONALES DESARROLLADOS**

#### **1. GeneraciÃ³n de Datos de Prueba**
- **Archivos**: `clientes.csv`, `productos.csv`, `sucursales.csv`, `ventas.csv`
- **CaracterÃ­sticas**: Datos realistas con relaciones consistentes
- **Volumen**: 50 registros distribuidos en 4 entidades

#### **2. Pipeline ETL Automatizado**
- **Script**: `create_tables_spark_simple.sql`
- **FunciÃ³n**: Carga inicial de datos RAW desde HDFS
- **TecnologÃ­a**: Spark SQL con vistas temporales

#### **3. TransformaciÃ³n Dimensional**
- **Script**: `create_dimensional_model.sql`
- **FunciÃ³n**: CreaciÃ³n de esquema estrella con claves sustitutas
- **OptimizaciÃ³n**: Formato Parquet para consultas rÃ¡pidas

#### **4. Capa de AnÃ¡lisis**
- **Script**: `analytical_queries.sql`
- **FunciÃ³n**: 5 consultas empresariales crÃ­ticas
- **Salida**: Insights para toma de decisiones

#### **5. VerificaciÃ³n y ValidaciÃ³n**
- **Script**: `verification_summary.sql`
- **FunciÃ³n**: ValidaciÃ³n de integridad y completitud
- **MÃ©tricas**: Conteos, estructuras, calidad de datos

---

### ğŸ“Š **RESULTADOS DE ANÃLISIS EMPRESARIAL**

#### **KPIs Principales Generados**

| **MÃ©trica** | **Valor** | **Insight Empresarial** |
|-------------|-----------|-------------------------|
| **Ventas Totales 2024** | $10,758 | Rendimiento anual sÃ³lido |
| **CategorÃ­a LÃ­der** | ElectrÃ³nicos (84%) | Foco en tecnologÃ­a |
| **RegiÃ³n Top** | Costa (54%) | ConcentraciÃ³n urbana |
| **Cliente VIP** | Juan PÃ©rez ($2,830) | Segmento Premium |

#### **AnÃ¡lisis por Dimensiones**

**ğŸ“ˆ Rendimiento por CategorÃ­a**
- ElectrÃ³nicos: $9,019 (84% del total)
- Muebles: $1,385 (13% del total)  
- IluminaciÃ³n: $354 (3% del total)

**ğŸŒ DistribuciÃ³n GeogrÃ¡fica**
- Costa: $5,769 (54% - Mayor densidad urbana)
- Sur: $4,193 (39% - Alto valor promedio)
- Sierra: $796 (7% - Oportunidad de crecimiento)

**ğŸ‘¥ SegmentaciÃ³n de Clientes**
- Premium: Mayor valor individual promedio
- EstÃ¡ndar: Mayor volumen de transacciones
- BÃ¡sico: Oportunidad de upselling

---

### ğŸš€ **GUÃA DE IMPLEMENTACIÃ“N**

#### **Prerequisitos del Sistema**
```bash
# Verificar instalaciones
hadoop version    # â†’ 3.4.1
spark-sql --version  # â†’ 3.5.0
jps              # â†’ Servicios Hadoop activos
```

#### **Secuencia de EjecuciÃ³n**
```bash
# 1. Inicializar infraestructura
start-dfs.sh && start-yarn.sh

# 2. Cargar datos RAW
spark-sql --conf spark.sql.catalogImplementation=in-memory \
  -f create_tables_spark_simple.sql

# 3. Construir modelo dimensional  
spark-sql --conf spark.sql.catalogImplementation=in-memory \
  -f create_dimensional_model.sql

# 4. Ejecutar anÃ¡lisis empresarial
spark-sql --conf spark.sql.catalogImplementation=in-memory \
  -f analytical_queries.sql

# 5. Validar implementaciÃ³n
spark-sql --conf spark.sql.catalogImplementation=in-memory \
  -f verification_summary.sql
```

---

### ğŸ’¡ **INNOVACIONES TÃ‰CNICAS IMPLEMENTADAS**

#### **Optimizaciones de Rendimiento**
- **Formato Parquet**: CompresiÃ³n columnar para consultas rÃ¡pidas
- **Particionamiento**: OrganizaciÃ³n eficiente de datos temporales  
- **Claves Sustitutas MD5**: Joins optimizados entre dimensiones
- **In-Memory Processing**: Spark SQL para anÃ¡lisis en tiempo real

#### **Escalabilidad Empresarial**
- **Arquitectura Modular**: FÃ¡cil adiciÃ³n de nuevas dimensiones
- **Data Lake Pattern**: SeparaciÃ³n RAW/PROCESSED para flexibilidad
- **Schema Evolution**: Soporte para cambios en estructura de datos
- **Multi-Format Support**: CSV, Parquet, futuro JSON/Avro

---

### ğŸ¯ **CASOS DE USO EMPRESARIAL**

#### **AnÃ¡lisis EstratÃ©gico**
- IdentificaciÃ³n de productos estrella
- OptimizaciÃ³n de inventario por regiÃ³n
- SegmentaciÃ³n avanzada de clientes
- Forecasting de ventas por temporada

#### **Operaciones TÃ¡cticas**
- Monitoreo de KPIs en tiempo real
- Alertas de rendimiento por sucursal
- AnÃ¡lisis de rentabilidad por producto
- OptimizaciÃ³n de rutas de distribuciÃ³n

---

### ğŸ“‹ **ENTREGABLES DEL PROYECTO**

| **Archivo** | **PropÃ³sito** | **TecnologÃ­a** |
|-------------|---------------|----------------|
| `*.csv` | Datos fuente empresariales | CSV estÃ¡ndar |
| `create_tables_spark_simple.sql` | Carga inicial RAW | Spark SQL |
| `create_dimensional_model.sql` | ETL dimensional | Spark SQL + Parquet |
| `analytical_queries.sql` | Consultas de negocio | Spark SQL Analytics |
| `verification_summary.sql` | ValidaciÃ³n de calidad | Spark SQL Testing |

---

### âœ… **ESTADO DEL PROYECTO**

**ğŸ‰ IMPLEMENTACIÃ“N COMPLETADA EXITOSAMENTE**

- âœ… **Infraestructura**: Hadoop cluster operativo
- âœ… **Data Pipeline**: ETL automatizado funcionando  
- âœ… **Modelo Dimensional**: Esquema estrella optimizado
- âœ… **Analytics**: 5 consultas empresariales validadas
- âœ… **DocumentaciÃ³n**: GuÃ­as tÃ©cnicas y de negocio completas

**ğŸ“ˆ Impacto Empresarial**: Sistema listo para producciÃ³n con capacidad de procesar volÃºmenes empresariales y generar insights crÃ­ticos para la toma de decisiones estratÃ©gicas.

---

### ğŸ”® **ROADMAP FUTURO**

#### **Fase 2 - ExpansiÃ³n**
- IntegraciÃ³n con fuentes de datos adicionales
- Dashboard interactivo con Apache Superset
- Alertas automÃ¡ticas por email/Slack
- API REST para consultas programÃ¡ticas

#### **Fase 3 - Machine Learning**
- Modelos predictivos de ventas
- AnÃ¡lisis de sentimiento de clientes  
- Recomendaciones personalizadas
- DetecciÃ³n de anomalÃ­as en tiempo real

---

**Desarrollado por**: Equipo de Data Engineering  
**Fecha**: Agosto 2024  
**VersiÃ³n**: 1.0 - ProducciÃ³n
